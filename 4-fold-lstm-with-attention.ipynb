{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\n\nfrom tqdm import tqdm\n\n# print(os.listdir(\"../input/\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d56c10e9171a51447ff0602d3c23c0a69982953"},"cell_type":"markdown","source":"## 数据预处理部分"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"[Step 1] Data Preprocessing...\")\n#导入训练数据，测试数据\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint('train set have %d sincere, %d insincere' % (train_df.groupby(['target']).size()[0], train_df.groupby(['target']).size()[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"032d1ff744e8f3719826ba7390851febc29364ab"},"cell_type":"code","source":"# puncts=['☹', 'Ź', 'Ż', 'ἰ', 'ή', 'Š', '＞', 'ξ','ฉ', 'ั', 'น', 'จ', 'ะ', 'ท', 'ำ', 'ใ', 'ห', '้', 'ด', 'ี', '่', 'ส', 'ุ', 'Π', 'प', 'ऊ', 'Ö', 'خ', 'ب', 'ஜ', 'ோ', 'ட', '', '「', 'ẽ', '½', '△', 'É', 'ķ', 'ï', '¿', 'ł', '북', '한', '¼', '∆', '≥', '⇒', '¬', '∨', 'č', 'š', '∫', 'ḥ', 'ā', 'ī', 'Ñ', 'à', '▾', 'Ω', '＾', 'ý', 'µ', '?', '!', '.', ',', '\"', '#', '$', '%', '\\\\', \"'\", '(', ')', '*', '+', '-', '/', ':', ';', '<', '=', '>', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', ' ', '“', '”', '’', 'é', 'á', '′', '…', 'ɾ', '̃', 'ɖ', 'ö', '–', '‘', 'ऋ', 'ॠ', 'ऌ', 'ॡ', 'ò', 'è', 'ù', 'â', 'ğ', 'म', 'ि', 'ल', 'ग', 'ई', 'क', 'े', 'ज', 'ो', 'ठ', 'ं', 'ड', 'Ž', 'ž', 'ó', '®', 'ê', 'ạ', 'ệ', '°', 'ص', 'و', 'ر', 'ü', '²', '₹', 'ú', '√', 'α', '→', 'ū', '—', '£', 'ä', '️', 'ø', '´', '×', 'í', 'ō', 'π', '÷', 'ʿ', '€', 'ñ', 'ç', 'へ', 'の', 'と', 'も', '↑', '∞', 'ʻ', '℅''ι', '•', 'ì', '−', 'л', 'я', 'д', 'ل', 'ك', 'م', 'ق', 'ا', '∈', '∩', '⊆', 'ã', 'अ', 'न', 'ु', 'स', '्', 'व', 'ा', 'र', 'त', '§', '℃', 'θ', '±', '≤', 'उ', 'द', 'य', 'ब', 'ट', '͡', '͜', 'ʖ', '⁴', '™', 'ć', 'ô', 'с', 'п', 'и', 'б', 'о', 'г', '≠', '∂', 'आ', 'ह', 'भ', 'ी', '³', 'च', '...', '⌚', '⟨', '⟩', '∖', '˂', 'ⁿ', '⅔', 'న', 'ీ', 'క', 'ె', 'ం', 'ద', 'ు', 'ా', 'గ', 'ర', 'ి', 'చ', 'র', 'ড়', 'ঢ়', 'સ', 'ં', 'ઘ', 'ર', 'ા', 'જ', '્', 'ય', 'ε', 'ν', 'τ', 'σ', 'ş', 'ś', 'س', 'ت', 'ط', 'ي', 'ع', 'ة', 'د', 'Å', '☺', 'ℇ', '❤', '♨', '✌', 'ﬁ', 'て', '„', 'Ā', 'ត', 'ើ', 'ប', 'ង', '្', 'អ', 'ូ', 'ន', 'ម', 'ា', 'ធ', 'យ', 'វ', 'ី', 'ខ', 'ល', 'ះ', 'ដ', 'រ', 'ក', 'ឃ', 'ញ', 'ឯ', 'ស', 'ំ', 'ព', 'ិ', 'ៃ', 'ទ', 'គ', '¢', 'つ', 'や', 'ค', 'ณ', 'ก', 'ล', 'ง', 'อ', 'ไ', 'ร', 'į', 'ی', 'ю', 'ʌ', 'ʊ', 'י', 'ה', 'ו', 'ד', 'ת', 'ᠠ', 'ᡳ', '᠌', 'ᠰ', 'ᠨ', 'ᡤ', 'ᡠ', 'ᡵ', 'ṭ', 'ế', 'ध', 'ड़', 'ß', '¸', 'ч',  'ễ', 'ộ', 'फ', 'μ', '⧼', '⧽', 'ম', 'হ', 'া', 'ব', 'ি', 'শ', '্', 'প', 'ত', 'ন', 'য়', 'স', 'চ', 'ছ', 'ে', 'ষ', 'য', '়', 'ট', 'উ', 'থ', 'ক', 'ῥ', 'ζ', 'ὤ', 'Ü', 'Δ', '내', '제', 'ʃ', 'ɸ', 'ợ', 'ĺ', 'º', 'ष', '♭', '़', '✅', '✓', 'ě', '∘', '¨', '″', 'İ', '⃗', '̂', 'æ', 'ɔ', '∑', '¾', 'Я', 'х', 'О', 'з', 'ف', 'ن', 'ḵ', 'Č', 'П', 'ь', 'В', 'Φ', 'ỵ', 'ɦ', 'ʏ', 'ɨ', 'ɛ', 'ʀ', 'ċ', 'օ', 'ʍ', 'ռ', 'ք', 'ʋ', '兰', 'ϵ', 'δ', 'Ľ', 'ɒ', 'î', 'Ἀ', 'χ', 'ῆ', 'ύ', 'ኤ', 'ል', 'ሮ', 'ኢ', 'የ', 'ኝ', 'ን', 'አ', 'ሁ', '≅', 'ϕ', '‑', 'ả', '￼', 'ֿ', 'か', 'く', 'れ', 'ő', '－', 'ș', 'ן', 'Γ', '∪', 'φ', 'ψ', '⊨', 'β', '∠', 'Ó', '«', '»', 'Í', 'க', 'வ', 'ா', 'ம', '≈', '⁰', '⁷', 'ấ', 'ũ', '눈', '치', 'ụ', 'å', '،', '＝', '（', '）', 'ə', 'ਨ', 'ਾ', 'ਮ', 'ੁ', '︠', '︡', 'ɑ', 'ː', 'λ', '∧', '∀', 'Ō', 'ㅜ', 'Ο', 'ς', 'ο', 'η', 'Σ', 'ण']\n# odd_chars=[ '大','能', '化', '生', '水', '谷', '精', '微', 'ル', 'ー', 'ジ', 'ュ', '支', '那', '¹', 'マ', 'リ', '仲', '直', 'り', 'し', 'た', '主', '席', '血', '⅓', '漢', '髪', '金', '茶', '訓', '読', '黒', 'ř', 'あ', 'わ', 'る', '胡', '南', '수', '능', '广', '电', '总', 'ί', '서', '로', '가', '를', '행', '복', '하', '게', '기', '乡', '故', '爾', '汝', '言', '得', '理', '让', '骂', '野', '比', 'び', '太', '後', '宮', '甄', '嬛', '傳', '做', '莫', '你', '酱', '紫', '甲', '骨', '陳', '宗', '陈', '什', '么', '说', '伊', '藤', '長', 'ﷺ', '僕', 'だ', 'け', 'が', '街', '◦', '火', '团', '表',  '看', '他', '顺', '眼', '中', '華', '民', '國', '許', '自', '東', '儿', '臣', '惶', '恐', 'っ', '木', 'ホ', 'ج', '教', '官', '국', '고', '등', '학', '교', '는', '몇', '시', '간', '업', '니', '本', '語', '上', '手', 'で', 'ね', '台', '湾', '最', '美', '风', '景', 'Î', '≡', '皎', '滢', '杨', '∛', '簡', '訊', '短', '送', '發', 'お', '早', 'う', '朝', 'ش', 'ه', '饭', '乱', '吃', '话', '讲', '男', '女', '授', '受', '亲', '好', '心', '没', '报', '攻', '克', '禮', '儀', '統', '已', '經', '失', '存', '٨', '八', '‛', '字', '：', '别', '高', '兴', '还', '几', '个', '条', '件', '呢', '觀', '《', '》', '記', '宋', '楚', '瑜', '孫', '瀛', '枚', '无', '挑', '剔', '聖', '部', '頭', '合', '約', 'ρ', '油', '腻', '邋', '遢', 'ٌ', 'Ä', '射', '籍', '贯', '老', '常', '谈', '族', '伟', '复', '平', '天', '下', '悠', '堵', '阻', '愛', '过', '会', '俄', '罗', '斯', '茹', '西', '亚', '싱', '관', '없', '어', '나', '이', '키', '夢', '彩', '蛋', '鰹', '節', '狐', '狸', '鳳', '凰', '露', '王', '晓', '菲', '恋', 'に', '落', 'ち', 'ら', 'よ', '悲', '反', '清', '復', '明', '肉', '希', '望', '沒', '公', '病', '配', '信', '開', '始', '日', '商', '品', '発', '売', '分', '子', '创', '意', '梦', '工', '坊', 'ک', 'پ', 'ڤ', '蘭', '花', '羡', '慕', '和', '嫉', '妒', '是', '样', 'ご', 'め', 'な', 'さ', 'い', 'す', 'み', 'ま', 'せ', 'ん', '音', '红', '宝', '书', '封', '柏', '荣', '江', '青', '鸡', '汤', '文', '粵', '拼', '寧', '可', '錯', '殺', '千', '絕', '放', '過', '」', '之', '勢', '请', '国', '知', '识', '产', '权', '局', '標', '點', '符', '號', '新', '年', '快', '乐', '学', '业', '进', '步', '身', '体', '健', '康', '们', '读', '我', '的', '翻', '译', '篇', '章', '欢', '迎', '入', '坑', '有', '毒', '黎', '氏', '玉', '英', '啧', '您', '这', '口', '味', '奇', '特', '也', '就', '罢', '了', '非', '要', '以', '此', '为', '依', '据', '对', '人', '家', '批', '判', '一', '番', '不', '地', '道', '啊', '谢', '六', '佬']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"1f19f64a56b791611d852f6dacb598e74c95ac68"},"cell_type":"code","source":"import re\n#构造词典数据\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n#清洗文本信息\ndef clean_text(x):\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\n#清洗数字数据\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9441cae69493b753d035b926f2dcb026d1e5c9d7"},"cell_type":"code","source":"#寻找错误的单词拼写\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium'\n                }\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea9c6fc42cf1e2503d571f6f8cf2ae4142d05b6b"},"cell_type":"code","source":"import operator \n\n#查看词典数据与embedding之间差异\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5268a8184338616365cbd00616053bff473aeabb"},"cell_type":"code","source":"from gensim.models import KeyedVectors\n\nnews_path = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\nsentences = train_df[\"question_text\"].apply(lambda x: x.split())\nto_remove = ['a','to','of','and']\nsentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\nvocab = build_vocab(sentences)\n\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)\noov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee996de3ec0a07954b701c06425d29356a6dc868"},"cell_type":"code","source":"max_len = 0\nmean_len = 0\nfor sentence in sentences:\n    mean_len += len(sentence)\n    if len(sentence) > max_len:\n        max_len = len(sentence)\nprint(\"the longest sentence has %d words\" % max_len)\nprint(\"sentence mean length has %d words\" % int(float(mean_len)/len(sentences)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7ecee5c61d1cc8dfe6cb6a83255671aaed1ca06"},"cell_type":"markdown","source":"## embedding处理部分"},{"metadata":{"trusted":true,"_uuid":"e7c564493a1e8ea0142a36836720f539799a1142"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nprint(\"[Step 2] Embedding...\")\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.01, random_state=2018, stratify = train_df['target'])\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 100000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 40 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X) + list(test_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c28645f0ee82af67f49df8a812da7888cd63ec4"},"cell_type":"code","source":"#构造embedding矩阵\nfrom gensim.models import KeyedVectors\n\ndef load_word2wac():\n    news_path = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n    embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)\n    return embeddings_index\n\ndef load_glove(word_index):\n    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n    \n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.005838499,0.48782197\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix\n\ndef load_para(word_index):\n    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = -0.0053247833,0.49346462\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27ccad5ac91a3cd584c83189a2f8ceac07d8e6e2"},"cell_type":"code","source":"#构造embedding矩阵\nword_dic = tokenizer.word_index\nembeddings_index1 = load_glove(word_dic)\n# embeddings_index2 = load_word2wac()\nembeddings_index2 = load_para(word_dic)\n\nembedding_matrix = np.mean([embeddings_index1, embeddings_index2], axis=0)\n\n###############################################################################################\ndel embeddings_index1, embeddings_index2\ngc.collect()\n###############################################################################################\n\nnp.shape(embedding_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5442a7171247b0601d85a3c71eb8260bb22b585"},"cell_type":"markdown","source":"## 网络部分"},{"metadata":{"trusted":true,"_uuid":"44eb5d863d64572f8e8df0fd0660046d1b44141c"},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Flatten, Dropout, Activation, Conv1D, MaxPooling1D, Embedding, CuDNNLSTM, Bidirectional\nfrom keras.layers.merge import Concatenate\nfrom keras.engine.topology import Layer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom keras import initializers, regularizers, constraints\nfrom keras import backend as K\nfrom keras.layers.normalization import BatchNormalization\n\nprint(\"[Step 3] Neural Network Training...\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f64441a102e40f7b4204e0bce997dd0bff9b6093"},"cell_type":"markdown","source":"### Attention Layer"},{"metadata":{"trusted":true,"_uuid":"7327e94ccbbbc1eeb6b8c4a7ef546d5df0f211a3"},"cell_type":"code","source":"# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\nfrom keras.engine.topology import Layer\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60fdfcd0e952ee0d5b2beba77366150c85c6f68b"},"cell_type":"markdown","source":"### callback"},{"metadata":{"trusted":true,"_uuid":"c683aa666e26db3d0d7d140d5265038b67bc1105"},"cell_type":"code","source":"from keras.callbacks import *\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n    \n\ndef f1(y_true, y_pred):\n    '''\n    metric from here \n    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n    '''\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0692555fa4af5a6e3e1e51e6f88ca9bc55b2edf4"},"cell_type":"markdown","source":"### CRF"},{"metadata":{"trusted":true,"_uuid":"c143f450b6a80d0e04aff9cd6bbfcac0f4c08e25"},"cell_type":"code","source":"from keras import backend as K\nfrom keras import activations\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.engine import Layer\nfrom keras.engine import InputSpec\nfrom keras.objectives import categorical_crossentropy\nfrom keras.objectives import sparse_categorical_crossentropy\n\n\nclass CRF(Layer):\n    \"\"\"An implementation of linear chain conditional random field (CRF).\n    An linear chain CRF is defined to maximize the following likelihood function:\n    $$ L(W, U, b; y_1, ..., y_n) := \\frac{1}{Z} \\sum_{y_1, ..., y_n} \\exp(-a_1' y_1 - a_n' y_n\n        - \\sum_{k=1^n}((f(x_k' W + b) y_k) + y_1' U y_2)), $$\n    where:\n        $Z$: normalization constant\n        $x_k, y_k$:  inputs and outputs\n    This implementation has two modes for optimization:\n    1. (`join mode`) optimized by maximizing join likelihood, which is optimal in theory of statistics.\n       Note that in this case, CRF must be the output/last layer.\n    2. (`marginal mode`) return marginal probabilities on each time step and optimized via composition\n       likelihood (product of marginal likelihood), i.e., using `categorical_crossentropy` loss.\n       Note that in this case, CRF can be either the last layer or an intermediate layer (though not explored).\n    For prediction (test phrase), one can choose either Viterbi best path (class indices) or marginal\n    probabilities if probabilities are needed. However, if one chooses *join mode* for training,\n    Viterbi output is typically better than marginal output, but the marginal output will still perform\n    reasonably close, while if *marginal mode* is used for training, marginal output usually performs\n    much better. The default behavior is set according to this observation.\n    In addition, this implementation supports masking and accepts either onehot or sparse target.\n    # Examples\n    ```python\n        model = Sequential()\n        model.add(Embedding(3001, 300, mask_zero=True)(X)\n        # use learn_mode = 'join', test_mode = 'viterbi', sparse_target = True (label indice output)\n        crf = CRF(10, sparse_target=True)\n        model.add(crf)\n        # crf.accuracy is default to Viterbi acc if using join-mode (default).\n        # One can add crf.marginal_acc if interested, but may slow down learning\n        model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])\n        # y must be label indices (with shape 1 at dim 3) here, since `sparse_target=True`\n        model.fit(x, y)\n        # prediction give onehot representation of Viterbi best path\n        y_hat = model.predict(x_test)\n    ```\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        learn_mode: Either 'join' or 'marginal'.\n            The former train the model by maximizing join likelihood while the latter\n            maximize the product of marginal likelihood over all time steps.\n        test_mode: Either 'viterbi' or 'marginal'.\n            The former is recommended and as default when `learn_mode = 'join'` and\n            gives one-hot representation of the best path at test (prediction) time,\n            while the latter is recommended and chosen as default when `learn_mode = 'marginal'`,\n            which produces marginal probabilities for each time step.\n        sparse_target: Boolean (default False) indicating if provided labels are one-hot or\n            indices (with shape 1 at dim 3).\n        use_boundary: Boolean (default True) indicating if trainable start-end chain energies\n            should be added to model.\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        chain_initializer: Initializer for the `chain_kernel` weights matrix,\n            used for the CRF chain energy.\n            (see [initializers](../initializers.md)).\n        boundary_initializer: Initializer for the `left_boundary`, 'right_boundary' weights vectors,\n            used for the start/left and end/right boundary energy.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        chain_regularizer: Regularizer function applied to\n            the `chain_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        boundary_regularizer: Regularizer function applied to\n            the 'left_boundary', 'right_boundary' weight vectors\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        chain_constraint: Constraint function applied to\n            the `chain_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        boundary_constraint: Constraint function applied to\n            the `left_boundary`, `right_boundary` weights vectors\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        input_dim: dimensionality of the input (integer).\n            This argument (or alternatively, the keyword argument `input_shape`)\n            is required when using this layer as the first layer in a model.\n        unroll: Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used.\n            Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n    # Input shape\n        3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n    # Output shape\n        3D tensor with shape `(nb_samples, timesteps, units)`.\n    # Masking\n        This layer supports masking for input data with a variable number\n        of timesteps. To introduce masks to your data,\n        use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n        set to `True`.\n    \"\"\"\n\n    def __init__(self, units,\n                 learn_mode='join',\n                 test_mode=None,\n                 sparse_target=False,\n                 use_boundary=True,\n                 use_bias=True,\n                 activation='linear',\n                 kernel_initializer='glorot_uniform',\n                 chain_initializer='orthogonal',\n                 bias_initializer='zeros',\n                 boundary_initializer='zeros',\n                 kernel_regularizer=None,\n                 chain_regularizer=None,\n                 boundary_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 chain_constraint=None,\n                 boundary_constraint=None,\n                 bias_constraint=None,\n                 input_dim=None,\n                 unroll=False,\n                 **kwargs):\n        super(CRF, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.units = units\n        self.learn_mode = learn_mode\n        assert self.learn_mode in ['join', 'marginal']\n        self.test_mode = test_mode\n        if self.test_mode is None:\n            self.test_mode = 'viterbi' if self.learn_mode == 'join' else 'marginal'\n        else:\n            assert self.test_mode in ['viterbi', 'marginal']\n        self.sparse_target = sparse_target\n        self.use_boundary = use_boundary\n        self.use_bias = use_bias\n\n        self.activation = activations.get(activation)\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.chain_initializer = initializers.get(chain_initializer)\n        self.boundary_initializer = initializers.get(boundary_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.chain_regularizer = regularizers.get(chain_regularizer)\n        self.boundary_regularizer = regularizers.get(boundary_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.chain_constraint = constraints.get(chain_constraint)\n        self.boundary_constraint = constraints.get(boundary_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.unroll = unroll\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(shape=input_shape)]\n        self.input_dim = input_shape[-1]\n\n        self.kernel = self.add_weight((self.input_dim, self.units),\n                                      name='kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        self.chain_kernel = self.add_weight((self.units, self.units),\n                                            name='chain_kernel',\n                                            initializer=self.chain_initializer,\n                                            regularizer=self.chain_regularizer,\n                                            constraint=self.chain_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight((self.units,),\n                                        name='bias',\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        if self.use_boundary:\n            self.left_boundary = self.add_weight((self.units,),\n                                                 name='left_boundary',\n                                                 initializer=self.boundary_initializer,\n                                                 regularizer=self.boundary_regularizer,\n                                                 constraint=self.boundary_constraint)\n            self.right_boundary = self.add_weight((self.units,),\n                                                  name='right_boundary',\n                                                  initializer=self.boundary_initializer,\n                                                  regularizer=self.boundary_regularizer,\n                                                  constraint=self.boundary_constraint)\n        self.built = True\n\n    def call(self, X, mask=None):\n        if mask is not None:\n            assert K.ndim(mask) == 2, 'Input mask to CRF must have dim 2 if not None'\n\n        if self.test_mode == 'viterbi':\n            test_output = self.viterbi_decoding(X, mask)\n        else:\n            test_output = self.get_marginal_prob(X, mask)\n\n        self.uses_learning_phase = True\n        if self.learn_mode == 'join':\n            train_output = K.zeros_like(K.dot(X, self.kernel))\n            out = K.in_train_phase(train_output, test_output)\n        else:\n            if self.test_mode == 'viterbi':\n                train_output = self.get_marginal_prob(X, mask)\n                out = K.in_train_phase(train_output, test_output)\n            else:\n                out = test_output\n        return out\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:2] + (self.units,)\n\n    def compute_mask(self, input, mask=None):\n        if mask is not None and self.learn_mode == 'join':\n            return K.any(mask, axis=1)\n        return mask\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'learn_mode': self.learn_mode,\n                  'test_mode': self.test_mode,\n                  'use_boundary': self.use_boundary,\n                  'use_bias': self.use_bias,\n                  'sparse_target': self.sparse_target,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'chain_initializer': initializers.serialize(self.chain_initializer),\n                  'boundary_initializer': initializers.serialize(self.boundary_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'activation': activations.serialize(self.activation),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'chain_regularizer': regularizers.serialize(self.chain_regularizer),\n                  'boundary_regularizer': regularizers.serialize(self.boundary_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'chain_constraint': constraints.serialize(self.chain_constraint),\n                  'boundary_constraint': constraints.serialize(self.boundary_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'input_dim': self.input_dim,\n                  'unroll': self.unroll}\n        base_config = super(CRF, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @property\n    def loss_function(self):\n        if self.learn_mode == 'join':\n            def loss(y_true, y_pred):\n                assert self._inbound_nodes, 'CRF has not connected to any layer.'\n                assert not self._outbound_nodes, 'When learn_model=\"join\", CRF must be the last layer.'\n                if self.sparse_target:\n                    y_true = K.one_hot(K.cast(y_true[:, :, 0], 'int32'), self.units)\n                X = self._inbound_nodes[0].input_tensors[0]\n                mask = self._inbound_nodes[0].input_masks[0]\n                nloglik = self.get_negative_log_likelihood(y_true, X, mask)\n                return nloglik\n            return loss\n        else:\n            if self.sparse_target:\n                return sparse_categorical_crossentropy\n            else:\n                return categorical_crossentropy\n\n    @property\n    def accuracy(self):\n        if self.test_mode == 'viterbi':\n            return self.viterbi_acc\n        else:\n            return self.marginal_acc\n\n    @staticmethod\n    def _get_accuracy(y_true, y_pred, mask, sparse_target=False):\n        y_pred = K.argmax(y_pred, -1)\n        if sparse_target:\n            y_true = K.cast(y_true[:, :, 0], K.dtype(y_pred))\n        else:\n            y_true = K.argmax(y_true, -1)\n        judge = K.cast(K.equal(y_pred, y_true), K.floatx())\n        if mask is None:\n            return K.mean(judge)\n        else:\n            mask = K.cast(mask, K.floatx())\n            return K.sum(judge * mask) / K.sum(mask)\n\n    @property\n    def viterbi_acc(self):\n        def acc(y_true, y_pred):\n            X = self._inbound_nodes[0].input_tensors[0]\n            mask = self._inbound_nodes[0].input_masks[0]\n            y_pred = self.viterbi_decoding(X, mask)\n            return self._get_accuracy(y_true, y_pred, mask, self.sparse_target)\n        acc.func_name = 'viterbi_acc'\n        return acc\n\n    @property\n    def marginal_acc(self):\n        def acc(y_true, y_pred):\n            X = self._inbound_nodes[0].input_tensors[0]\n            mask = self._inbound_nodes[0].input_masks[0]\n            y_pred = self.get_marginal_prob(X, mask)\n            return self._get_accuracy(y_true, y_pred, mask, self.sparse_target)\n        acc.func_name = 'marginal_acc'\n        return acc\n\n    @staticmethod\n    def softmaxNd(x, axis=-1):\n        m = K.max(x, axis=axis, keepdims=True)\n        exp_x = K.exp(x - m)\n        prob_x = exp_x / K.sum(exp_x, axis=axis, keepdims=True)\n        return prob_x\n\n    @staticmethod\n    def shift_left(x, offset=1):\n        assert offset > 0\n        return K.concatenate([x[:, offset:], K.zeros_like(x[:, :offset])], axis=1)\n\n    @staticmethod\n    def shift_right(x, offset=1):\n        assert offset > 0\n        return K.concatenate([K.zeros_like(x[:, :offset]), x[:, :-offset]], axis=1)\n\n    def add_boundary_energy(self, energy, mask, start, end):\n        start = K.expand_dims(K.expand_dims(start, 0), 0)\n        end = K.expand_dims(K.expand_dims(end, 0), 0)\n        if mask is None:\n            energy = K.concatenate([energy[:, :1, :] + start, energy[:, 1:, :]], axis=1)\n            energy = K.concatenate([energy[:, :-1, :], energy[:, -1:, :] + end], axis=1)\n        else:\n            mask = K.expand_dims(K.cast(mask, K.floatx()))\n            start_mask = K.cast(K.greater(mask, self.shift_right(mask)), K.floatx())\n            end_mask = K.cast(K.greater(self.shift_left(mask), mask), K.floatx())\n            energy = energy + start_mask * start\n            energy = energy + end_mask * end\n        return energy\n\n    def get_log_normalization_constant(self, input_energy, mask, **kwargs):\n        \"\"\"Compute logarithm of the normalization constant Z, where\n        Z = sum exp(-E) -> logZ = log sum exp(-E) =: -nlogZ\n        \"\"\"\n        # should have logZ[:, i] == logZ[:, j] for any i, j\n        logZ = self.recursion(input_energy, mask, return_sequences=False, **kwargs)\n        return logZ[:, 0]\n\n    def get_energy(self, y_true, input_energy, mask):\n        \"\"\"Energy = a1' y1 + u1' y1 + y1' U y2 + u2' y2 + y2' U y3 + u3' y3 + an' y3\n        \"\"\"\n        input_energy = K.sum(input_energy * y_true, 2)  # (B, T)\n        chain_energy = K.sum(K.dot(y_true[:, :-1, :], self.chain_kernel) * y_true[:, 1:, :], 2)  # (B, T-1)\n\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            chain_mask = mask[:, :-1] * mask[:, 1:]  # (B, T-1), mask[:,:-1]*mask[:,1:] makes it work with any padding\n            input_energy = input_energy * mask\n            chain_energy = chain_energy * chain_mask\n        total_energy = K.sum(input_energy, -1) + K.sum(chain_energy, -1)  # (B, )\n\n        return total_energy\n\n    def get_negative_log_likelihood(self, y_true, X, mask):\n        \"\"\"Compute the loss, i.e., negative log likelihood (normalize by number of time steps)\n           likelihood = 1/Z * exp(-E) ->  neg_log_like = - log(1/Z * exp(-E)) = logZ + E\n        \"\"\"\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask, self.left_boundary, self.right_boundary)\n        energy = self.get_energy(y_true, input_energy, mask)\n        logZ = self.get_log_normalization_constant(input_energy, mask, input_length=K.int_shape(X)[1])\n        nloglik = logZ + energy\n        if mask is not None:\n            nloglik = nloglik / K.sum(K.cast(mask, K.floatx()), 1)\n        else:\n            nloglik = nloglik / K.cast(K.shape(X)[1], K.floatx())\n        return nloglik\n\n    def step(self, input_energy_t, states, return_logZ=True):\n        # not in the following  `prev_target_val` has shape = (B, F)\n        # where B = batch_size, F = output feature dim\n        # Note: `i` is of float32, due to the behavior of `K.rnn`\n        prev_target_val, i, chain_energy = states[:3]\n        t = K.cast(i[0, 0], dtype='int32')\n        if len(states) > 3:\n            if K.backend() == 'theano':\n                m = states[3][:, t:(t + 2)]\n            else:\n                m = K.tf.slice(states[3], [0, t], [-1, 2])\n            input_energy_t = input_energy_t * K.expand_dims(m[:, 0])\n            chain_energy = chain_energy * K.expand_dims(K.expand_dims(m[:, 0] * m[:, 1]))  # (1, F, F)*(B, 1, 1) -> (B, F, F)\n        if return_logZ:\n            energy = chain_energy + K.expand_dims(input_energy_t - prev_target_val, 2)  # shapes: (1, B, F) + (B, F, 1) -> (B, F, F)\n            new_target_val = K.logsumexp(-energy, 1)  # shapes: (B, F)\n            return new_target_val, [new_target_val, i + 1]\n        else:\n            energy = chain_energy + K.expand_dims(input_energy_t + prev_target_val, 2)\n            min_energy = K.min(energy, 1)\n            argmin_table = K.cast(K.argmin(energy, 1), K.floatx())  # cast for tf-version `K.rnn`\n            return argmin_table, [min_energy, i + 1]\n\n    def recursion(self, input_energy, mask=None, go_backwards=False, return_sequences=True, return_logZ=True, input_length=None):\n        \"\"\"Forward (alpha) or backward (beta) recursion\n        If `return_logZ = True`, compute the logZ, the normalization constant:\n        \\[ Z = \\sum_{y1, y2, y3} exp(-E) # energy\n          = \\sum_{y1, y2, y3} exp(-(u1' y1 + y1' W y2 + u2' y2 + y2' W y3 + u3' y3))\n          = sum_{y2, y3} (exp(-(u2' y2 + y2' W y3 + u3' y3)) sum_{y1} exp(-(u1' y1' + y1' W y2))) \\]\n        Denote:\n            \\[ S(y2) := sum_{y1} exp(-(u1' y1 + y1' W y2)), \\]\n            \\[ Z = sum_{y2, y3} exp(log S(y2) - (u2' y2 + y2' W y3 + u3' y3)) \\]\n            \\[ logS(y2) = log S(y2) = log_sum_exp(-(u1' y1' + y1' W y2)) \\]\n        Note that:\n              yi's are one-hot vectors\n              u1, u3: boundary energies have been merged\n        If `return_logZ = False`, compute the Viterbi's best path lookup table.\n        \"\"\"\n        chain_energy = self.chain_kernel\n        chain_energy = K.expand_dims(chain_energy, 0)  # shape=(1, F, F): F=num of output features. 1st F is for t-1, 2nd F for t\n        prev_target_val = K.zeros_like(input_energy[:, 0, :])  # shape=(B, F), dtype=float32\n\n        if go_backwards:\n            input_energy = K.reverse(input_energy, 1)\n            if mask is not None:\n                mask = K.reverse(mask, 1)\n\n        initial_states = [prev_target_val, K.zeros_like(prev_target_val[:, :1])]\n        constants = [chain_energy]\n\n        if mask is not None:\n            mask2 = K.cast(K.concatenate([mask, K.zeros_like(mask[:, :1])], axis=1), K.floatx())\n            constants.append(mask2)\n\n        def _step(input_energy_i, states):\n            return self.step(input_energy_i, states, return_logZ)\n\n        target_val_last, target_val_seq, _ = K.rnn(_step, input_energy, initial_states, constants=constants,\n                                                   input_length=input_length, unroll=self.unroll)\n\n        if return_sequences:\n            if go_backwards:\n                target_val_seq = K.reverse(target_val_seq, 1)\n            return target_val_seq\n        else:\n            return target_val_last\n\n    def forward_recursion(self, input_energy, **kwargs):\n        return self.recursion(input_energy, **kwargs)\n\n    def backward_recursion(self, input_energy, **kwargs):\n        return self.recursion(input_energy, go_backwards=True, **kwargs)\n\n    def get_marginal_prob(self, X, mask=None):\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask, self.left_boundary, self.right_boundary)\n        input_length = K.int_shape(X)[1]\n        alpha = self.forward_recursion(input_energy, mask=mask, input_length=input_length)\n        beta = self.backward_recursion(input_energy, mask=mask, input_length=input_length)\n        if mask is not None:\n            input_energy = input_energy * K.expand_dims(K.cast(mask, K.floatx()))\n        margin = -(self.shift_right(alpha) + input_energy + self.shift_left(beta))\n        return self.softmaxNd(margin)\n\n    def viterbi_decoding(self, X, mask=None):\n        input_energy = self.activation(K.dot(X, self.kernel) + self.bias)\n        if self.use_boundary:\n            input_energy = self.add_boundary_energy(input_energy, mask, self.left_boundary, self.right_boundary)\n\n        argmin_tables = self.recursion(input_energy, mask, return_logZ=False)\n        argmin_tables = K.cast(argmin_tables, 'int32')\n\n        # backward to find best path, `initial_best_idx` can be any, as all elements in the last argmin_table are the same\n        argmin_tables = K.reverse(argmin_tables, 1)\n        initial_best_idx = [K.expand_dims(argmin_tables[:, 0, 0])]  # matrix instead of vector is required by tf `K.rnn`\n        if K.backend() == 'theano':\n            initial_best_idx = [K.T.unbroadcast(initial_best_idx[0], 1)]\n\n        def gather_each_row(params, indices):\n            n = K.shape(indices)[0]\n            if K.backend() == 'theano':\n                return params[K.T.arange(n), indices]\n            else:\n                indices = K.transpose(K.stack([K.tf.range(n), indices]))\n                return K.tf.gather_nd(params, indices)\n\n        def find_path(argmin_table, best_idx):\n            next_best_idx = gather_each_row(argmin_table, best_idx[0][:, 0])\n            next_best_idx = K.expand_dims(next_best_idx)\n            if K.backend() == 'theano':\n                next_best_idx = K.T.unbroadcast(next_best_idx, 1)\n            return next_best_idx, [next_best_idx]\n\n        _, best_paths, _ = K.rnn(find_path, argmin_tables, initial_best_idx, input_length=K.int_shape(X)[1], unroll=self.unroll)\n        best_paths = K.reverse(best_paths, 1)\n        best_paths = K.squeeze(best_paths, 2)\n\n        return K.one_hot(best_paths, self.units)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2679d539b092a8e2d0923d669408cbf2eb8382e6"},"cell_type":"markdown","source":"### CNN - kFold"},{"metadata":{"trusted":true,"_uuid":"7ec3e22a83c53eb9f2951f73b522465404ea67bb"},"cell_type":"code","source":"def simple_LSTM():\n    '''\n    RNN\n    [embedding - CNN - maxpooling - full_connect]\n    '''\n    _input = Input(shape=(maxlen,))\n    x = Embedding(np.shape(embedding_matrix)[0], embed_size, input_length=maxlen, weights=[embedding_matrix], trainable=False)(_input)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(x)\n    x = Attention(maxlen)(x)\n    x = Dense(16)(x)\n    x = Dropout(0.2)(x)\n    x = Dense(4)(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=_input, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print (model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b3d2fb6713e4a00c10d4be385da6630f7d3e0f4"},"cell_type":"code","source":"def BN_LSTM():\n    '''\n    RNN\n    [embedding - CNN - maxpooling - full_connect]\n    '''\n    _input = Input(shape=(maxlen,))\n    x = Embedding(np.shape(embedding_matrix)[0], embed_size, input_length=maxlen, weights=[embedding_matrix], trainable=False)(_input)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(x)\n    x = Attention(maxlen)(x)\n    x = Dense(32)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(16)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=_input, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print (model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cfa5bd6a62f8aef5d46df4f59b0e1e1ba4ba8f5"},"cell_type":"code","source":"def LSTM_CNN_CRF():\n    '''\n    RNN\n    [embedding - CNN - maxpooling - full_connect]\n    '''\n    filter_sizes = (2, 3)\n    _input = Input(shape=(maxlen,))\n    x = Embedding(np.shape(embedding_matrix)[0], embed_size, input_length=maxlen, weights=[embedding_matrix], trainable=False)(_input)\n    conv_blocks = []\n    conv_blocks.append(MaxPooling1D(2)(x))\n    for sz in filter_sizes:\n        conv = Conv1D(filters=sz * 10, kernel_size=sz, padding='same')(x)\n        conv = Dropout(0.1)(conv)\n        conv = MaxPooling1D(2)(conv)\n#         conv = Flatten()(conv)\n        conv_blocks.append(conv)\n    x = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n#     x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(x)\n    crf = CRF(32,learn_mode='marginal',unroll=True)\n    x = crf(x)\n    x = Attention(int(maxlen/2))(x)\n#     x = Flatten()(x)\n    x = Dense(16)(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=_input, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print (model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e70da91ef4035a327b6e63827f96744a7d4ea48d"},"cell_type":"code","source":"clr = CyclicLR(base_lr=0.001, max_lr=0.002,\n               step_size=300., mode='exp_range',\n               gamma=0.99994)\n\nk = 5\nkfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=1029)\nresults = []\nthreshs = []\nfor train, test in kfold.split(train_X, train_y):\n#     model = simple_LSTM()\n    model = LSTM_CNN_CRF()\n    model.fit(train_X[train], train_y[train], epochs=3, batch_size=512, validation_data=(train_X[test], train_y[test]))\n#     model.fit(train_X[train], train_y[train], epochs=3, batch_size=512, validation_data=(train_X[test], train_y[test]), callbacks = [clr,])\n    pred_part = model.predict(train_X[test])\n    \n    best_thresh = 0.5\n    best_score = 0.0\n    for thresh in tqdm(np.arange(0.1, 0.501, 0.01)):\n        thresh = np.round(thresh, 2)\n        y_te = (np.array(pred_part) > thresh).astype(np.int)\n        y_te = y_te.ravel()\n        score = f1_score(y_te, train_y[test])\n        if score > best_score:\n            best_thresh = thresh\n            best_score = score\n    threshs.append(best_thresh)\n    print(\"Part best_thresh: {:.4f}\".format(best_thresh))\n    print(\"Part F1 Score: {:.4f}\".format(best_score))\n    \n    results.append(model.predict(test_X).ravel())\n    \npred_vals = np.mean(np.array(results),axis=0)\nbest_thresh = np.mean(np.array(threshs))\nbest_thresh = np.round(best_thresh, 2)\ny_te = (pred_vals > best_thresh).astype(np.int)\ny_te = y_te.ravel()\nprint(\"Val best_thresh: {:.4f}\".format(best_thresh))\nprint(\"Val F1 Score: {:.4f}\".format(score))\n\nsubmit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fd402d9d69890a2d1a983115e10ddd070c6c082"},"cell_type":"markdown","source":"## 提交文件"},{"metadata":{"trusted":true,"_uuid":"6d85689405e33814c4a5dd88089a167762841393"},"cell_type":"code","source":"# print(\"[Step 4] Submit Results...\")\n\n# all_preds = model.predict(test_X)\n# y_te = (np.array(all_preds) > thresh).astype(np.int)\n\n# submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te.ravel()})\n# submit_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8c02386b20e55af58d86d4df6a2429712cd19ff"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e688eb56f89c65ddb89686e99d9e9b66883b48f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}