{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\n\nfrom tqdm import tqdm\n\n# print(os.listdir(\"../input/\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d56c10e9171a51447ff0602d3c23c0a69982953"},"cell_type":"markdown","source":"## 数据预处理部分"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"[Step 1] Data Preprocessing...\")\n#导入训练数据，测试数据\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint('train set have %d sincere, %d insincere' % (train_df.groupby(['target']).size()[0], train_df.groupby(['target']).size()[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"032d1ff744e8f3719826ba7390851febc29364ab"},"cell_type":"code","source":"# puncts=['☹', 'Ź', 'Ż', 'ἰ', 'ή', 'Š', '＞', 'ξ','ฉ', 'ั', 'น', 'จ', 'ะ', 'ท', 'ำ', 'ใ', 'ห', '้', 'ด', 'ี', '่', 'ส', 'ุ', 'Π', 'प', 'ऊ', 'Ö', 'خ', 'ب', 'ஜ', 'ோ', 'ட', '', '「', 'ẽ', '½', '△', 'É', 'ķ', 'ï', '¿', 'ł', '북', '한', '¼', '∆', '≥', '⇒', '¬', '∨', 'č', 'š', '∫', 'ḥ', 'ā', 'ī', 'Ñ', 'à', '▾', 'Ω', '＾', 'ý', 'µ', '?', '!', '.', ',', '\"', '#', '$', '%', '\\\\', \"'\", '(', ')', '*', '+', '-', '/', ':', ';', '<', '=', '>', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', ' ', '“', '”', '’', 'é', 'á', '′', '…', 'ɾ', '̃', 'ɖ', 'ö', '–', '‘', 'ऋ', 'ॠ', 'ऌ', 'ॡ', 'ò', 'è', 'ù', 'â', 'ğ', 'म', 'ि', 'ल', 'ग', 'ई', 'क', 'े', 'ज', 'ो', 'ठ', 'ं', 'ड', 'Ž', 'ž', 'ó', '®', 'ê', 'ạ', 'ệ', '°', 'ص', 'و', 'ر', 'ü', '²', '₹', 'ú', '√', 'α', '→', 'ū', '—', '£', 'ä', '️', 'ø', '´', '×', 'í', 'ō', 'π', '÷', 'ʿ', '€', 'ñ', 'ç', 'へ', 'の', 'と', 'も', '↑', '∞', 'ʻ', '℅''ι', '•', 'ì', '−', 'л', 'я', 'д', 'ل', 'ك', 'م', 'ق', 'ا', '∈', '∩', '⊆', 'ã', 'अ', 'न', 'ु', 'स', '्', 'व', 'ा', 'र', 'त', '§', '℃', 'θ', '±', '≤', 'उ', 'द', 'य', 'ब', 'ट', '͡', '͜', 'ʖ', '⁴', '™', 'ć', 'ô', 'с', 'п', 'и', 'б', 'о', 'г', '≠', '∂', 'आ', 'ह', 'भ', 'ी', '³', 'च', '...', '⌚', '⟨', '⟩', '∖', '˂', 'ⁿ', '⅔', 'న', 'ీ', 'క', 'ె', 'ం', 'ద', 'ు', 'ా', 'గ', 'ర', 'ి', 'చ', 'র', 'ড়', 'ঢ়', 'સ', 'ં', 'ઘ', 'ર', 'ા', 'જ', '્', 'ય', 'ε', 'ν', 'τ', 'σ', 'ş', 'ś', 'س', 'ت', 'ط', 'ي', 'ع', 'ة', 'د', 'Å', '☺', 'ℇ', '❤', '♨', '✌', 'ﬁ', 'て', '„', 'Ā', 'ត', 'ើ', 'ប', 'ង', '្', 'អ', 'ូ', 'ន', 'ម', 'ា', 'ធ', 'យ', 'វ', 'ី', 'ខ', 'ល', 'ះ', 'ដ', 'រ', 'ក', 'ឃ', 'ញ', 'ឯ', 'ស', 'ំ', 'ព', 'ិ', 'ៃ', 'ទ', 'គ', '¢', 'つ', 'や', 'ค', 'ณ', 'ก', 'ล', 'ง', 'อ', 'ไ', 'ร', 'į', 'ی', 'ю', 'ʌ', 'ʊ', 'י', 'ה', 'ו', 'ד', 'ת', 'ᠠ', 'ᡳ', '᠌', 'ᠰ', 'ᠨ', 'ᡤ', 'ᡠ', 'ᡵ', 'ṭ', 'ế', 'ध', 'ड़', 'ß', '¸', 'ч',  'ễ', 'ộ', 'फ', 'μ', '⧼', '⧽', 'ম', 'হ', 'া', 'ব', 'ি', 'শ', '্', 'প', 'ত', 'ন', 'য়', 'স', 'চ', 'ছ', 'ে', 'ষ', 'য', '়', 'ট', 'উ', 'থ', 'ক', 'ῥ', 'ζ', 'ὤ', 'Ü', 'Δ', '내', '제', 'ʃ', 'ɸ', 'ợ', 'ĺ', 'º', 'ष', '♭', '़', '✅', '✓', 'ě', '∘', '¨', '″', 'İ', '⃗', '̂', 'æ', 'ɔ', '∑', '¾', 'Я', 'х', 'О', 'з', 'ف', 'ن', 'ḵ', 'Č', 'П', 'ь', 'В', 'Φ', 'ỵ', 'ɦ', 'ʏ', 'ɨ', 'ɛ', 'ʀ', 'ċ', 'օ', 'ʍ', 'ռ', 'ք', 'ʋ', '兰', 'ϵ', 'δ', 'Ľ', 'ɒ', 'î', 'Ἀ', 'χ', 'ῆ', 'ύ', 'ኤ', 'ል', 'ሮ', 'ኢ', 'የ', 'ኝ', 'ን', 'አ', 'ሁ', '≅', 'ϕ', '‑', 'ả', '￼', 'ֿ', 'か', 'く', 'れ', 'ő', '－', 'ș', 'ן', 'Γ', '∪', 'φ', 'ψ', '⊨', 'β', '∠', 'Ó', '«', '»', 'Í', 'க', 'வ', 'ா', 'ம', '≈', '⁰', '⁷', 'ấ', 'ũ', '눈', '치', 'ụ', 'å', '،', '＝', '（', '）', 'ə', 'ਨ', 'ਾ', 'ਮ', 'ੁ', '︠', '︡', 'ɑ', 'ː', 'λ', '∧', '∀', 'Ō', 'ㅜ', 'Ο', 'ς', 'ο', 'η', 'Σ', 'ण']\n# odd_chars=[ '大','能', '化', '生', '水', '谷', '精', '微', 'ル', 'ー', 'ジ', 'ュ', '支', '那', '¹', 'マ', 'リ', '仲', '直', 'り', 'し', 'た', '主', '席', '血', '⅓', '漢', '髪', '金', '茶', '訓', '読', '黒', 'ř', 'あ', 'わ', 'る', '胡', '南', '수', '능', '广', '电', '总', 'ί', '서', '로', '가', '를', '행', '복', '하', '게', '기', '乡', '故', '爾', '汝', '言', '得', '理', '让', '骂', '野', '比', 'び', '太', '後', '宮', '甄', '嬛', '傳', '做', '莫', '你', '酱', '紫', '甲', '骨', '陳', '宗', '陈', '什', '么', '说', '伊', '藤', '長', 'ﷺ', '僕', 'だ', 'け', 'が', '街', '◦', '火', '团', '表',  '看', '他', '顺', '眼', '中', '華', '民', '國', '許', '自', '東', '儿', '臣', '惶', '恐', 'っ', '木', 'ホ', 'ج', '教', '官', '국', '고', '등', '학', '교', '는', '몇', '시', '간', '업', '니', '本', '語', '上', '手', 'で', 'ね', '台', '湾', '最', '美', '风', '景', 'Î', '≡', '皎', '滢', '杨', '∛', '簡', '訊', '短', '送', '發', 'お', '早', 'う', '朝', 'ش', 'ه', '饭', '乱', '吃', '话', '讲', '男', '女', '授', '受', '亲', '好', '心', '没', '报', '攻', '克', '禮', '儀', '統', '已', '經', '失', '存', '٨', '八', '‛', '字', '：', '别', '高', '兴', '还', '几', '个', '条', '件', '呢', '觀', '《', '》', '記', '宋', '楚', '瑜', '孫', '瀛', '枚', '无', '挑', '剔', '聖', '部', '頭', '合', '約', 'ρ', '油', '腻', '邋', '遢', 'ٌ', 'Ä', '射', '籍', '贯', '老', '常', '谈', '族', '伟', '复', '平', '天', '下', '悠', '堵', '阻', '愛', '过', '会', '俄', '罗', '斯', '茹', '西', '亚', '싱', '관', '없', '어', '나', '이', '키', '夢', '彩', '蛋', '鰹', '節', '狐', '狸', '鳳', '凰', '露', '王', '晓', '菲', '恋', 'に', '落', 'ち', 'ら', 'よ', '悲', '反', '清', '復', '明', '肉', '希', '望', '沒', '公', '病', '配', '信', '開', '始', '日', '商', '品', '発', '売', '分', '子', '创', '意', '梦', '工', '坊', 'ک', 'پ', 'ڤ', '蘭', '花', '羡', '慕', '和', '嫉', '妒', '是', '样', 'ご', 'め', 'な', 'さ', 'い', 'す', 'み', 'ま', 'せ', 'ん', '音', '红', '宝', '书', '封', '柏', '荣', '江', '青', '鸡', '汤', '文', '粵', '拼', '寧', '可', '錯', '殺', '千', '絕', '放', '過', '」', '之', '勢', '请', '国', '知', '识', '产', '权', '局', '標', '點', '符', '號', '新', '年', '快', '乐', '学', '业', '进', '步', '身', '体', '健', '康', '们', '读', '我', '的', '翻', '译', '篇', '章', '欢', '迎', '入', '坑', '有', '毒', '黎', '氏', '玉', '英', '啧', '您', '这', '口', '味', '奇', '特', '也', '就', '罢', '了', '非', '要', '以', '此', '为', '依', '据', '对', '人', '家', '批', '判', '一', '番', '不', '地', '道', '啊', '谢', '六', '佬']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"1f19f64a56b791611d852f6dacb598e74c95ac68"},"cell_type":"code","source":"import re\n#构造词典数据\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n#清洗文本信息\ndef clean_text(x):\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\n\n#清洗数字数据\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9441cae69493b753d035b926f2dcb026d1e5c9d7"},"cell_type":"code","source":"#寻找错误的单词拼写\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'wwii':'world war 2',\n                'citicise':'criticize',\n                'instagram': 'social medium',\n                'whatsapp': 'social medium',\n                'snapchat': 'social medium'\n                }\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea9c6fc42cf1e2503d571f6f8cf2ae4142d05b6b"},"cell_type":"code","source":"import operator \n\n#查看词典数据与embedding之间差异\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5268a8184338616365cbd00616053bff473aeabb"},"cell_type":"code","source":"from gensim.models import KeyedVectors\n\nnews_path = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\nsentences = train_df[\"question_text\"].apply(lambda x: x.split())\nto_remove = ['a','to','of','and']\nsentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\nvocab = build_vocab(sentences)\n\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)\noov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee996de3ec0a07954b701c06425d29356a6dc868"},"cell_type":"code","source":"max_len = 0\nmean_len = 0\nfor sentence in sentences:\n    mean_len += len(sentence)\n    if len(sentence) > max_len:\n        max_len = len(sentence)\nprint(\"the longest sentence has %d words\" % max_len)\nprint(\"sentence mean length has %d words\" % int(float(mean_len)/len(sentences)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7ecee5c61d1cc8dfe6cb6a83255671aaed1ca06"},"cell_type":"markdown","source":"## embedding处理部分"},{"metadata":{"trusted":true,"_uuid":"e7c564493a1e8ea0142a36836720f539799a1142"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nprint(\"[Step 2] Embedding...\")\n\ntrain_df, val_df = train_test_split(train_df, test_size=0.01, random_state=2018, stratify = train_df['target'])\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 100000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X) + list(test_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c28645f0ee82af67f49df8a812da7888cd63ec4"},"cell_type":"code","source":"#构造embedding矩阵\nfrom gensim.models import KeyedVectors\n\ndef load_word2wac():\n    news_path = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n    embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)\n    return embeddings_index\n\ndef load_glove(word_dic):\n    embeddings_index = {}\n    glove_f = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\n    for line in tqdm(glove_f):\n        values = line.split()\n        if len(values) == 0:\n            continue\n        try:\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n        except:\n            continue\n    glove_f.close()\n    return embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27ccad5ac91a3cd584c83189a2f8ceac07d8e6e2"},"cell_type":"code","source":"#构造embedding矩阵\nword_dic = tokenizer.word_index\nembeddings_index = load_glove(word_dic)\n# embeddings_index = load_word2wac(word_dic)\nembedding_matrix = []\nfor key, value in tqdm(word_dic.items()):\n    try:\n        embedding_matrix.append(embeddings_index[key])\n    except:\n        embedding_matrix.append([0 for i in range(300)])\nembedding_matrix = np.array(embedding_matrix)\n\n###############################################################################################\ndel embeddings_index\ngc.collect()\n###############################################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5442a7171247b0601d85a3c71eb8260bb22b585"},"cell_type":"markdown","source":"## 网络部分"},{"metadata":{"trusted":true,"_uuid":"44eb5d863d64572f8e8df0fd0660046d1b44141c"},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Flatten, Dropout, Activation, Conv1D, MaxPooling1D, Embedding\nfrom keras.layers.merge import Concatenate\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nprint(\"[Step 3] Neural Network Training...\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f64441a102e40f7b4204e0bce997dd0bff9b6093"},"cell_type":"markdown","source":"### Attention Layer"},{"metadata":{"trusted":true,"_uuid":"7327e94ccbbbc1eeb6b8c4a7ef546d5df0f211a3"},"cell_type":"code","source":"# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\nfrom keras.engine.topology import Layer\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd8f96bebf5deb41636bc05175ac9715a4fac7f4"},"cell_type":"markdown","source":"### simple CNN"},{"metadata":{"trusted":true,"_uuid":"c174d1877802c9a1b4bacd1449de2c4173c9df06"},"cell_type":"code","source":"def simple_CNN():\n    '''\n    CNN\n    [embedding - CNN - maxpooling - full_connect]\n    '''\n    _input = Input(shape=(maxlen,))\n    x = Embedding(len(word_dic), embed_size, input_length=maxlen, weights=[embedding_matrix], trainable=False)(_input)\n    x = Conv1D(filters=32, kernel_size=3, padding='same')(x)\n    x = MaxPooling1D(2)(x)\n    x = Flatten()(x)\n    x = Dense(500)(x)\n    x = Dropout(0.5)(x)\n    x = Dense(50)(x)\n    x = Dropout(0.8)(x)\n    x = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=_input, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n#     print (model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"618be5711157206cc65d52f1fa86305af1477e66"},"cell_type":"markdown","source":"### CNN - concat"},{"metadata":{"trusted":true,"_uuid":"f7cfd6a47123b935d825caeeba850a985438729c"},"cell_type":"code","source":"def concat_CNN():\n    '''\n    CNN\n    [embedding - CNN - CNN - concat - maxpooling - full_connect]\n    '''\n    filter_sizes = (2, 3)\n    _input = Input(shape=(maxlen,))\n    x = Embedding(len(word_dic), embed_size, input_length=maxlen, weights=[embedding_matrix], trainable=False)(_input)\n    conv_blocks = []\n    for sz in filter_sizes:\n        conv = Conv1D(filters=32, kernel_size=sz, padding='same')(x)\n#         conv = Attention(maxlen)(conv)\n        conv = MaxPooling1D(2)(conv)\n        conv = Flatten()(conv)\n        conv_blocks.append(conv)\n    x = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n    x = Dense(500)(x)\n    x = Dropout(0.5)(x)\n    x = Dense(50)(x)\n    x = Dropout(0.8)(x)\n    x = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=_input, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print (model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2679d539b092a8e2d0923d669408cbf2eb8382e6"},"cell_type":"markdown","source":"### CNN - kFold"},{"metadata":{"trusted":true,"_uuid":"e70da91ef4035a327b6e63827f96744a7d4ea48d"},"cell_type":"code","source":"# def concat_CNN():\n#     '''\n#     CNN\n#     [embedding - CNN - CNN - concat - maxpooling - full_connect]\n#     '''\n# k = 4\n# kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=7)\n# results = []\n# for train, test in kfold.split(train_X, train_y):\n#     filter_sizes = (2, 3)\n#     _input = Input(shape=(maxlen,))\n#     x = Embedding(len(word_dic), embed_size, input_length=maxlen, weights=[embedding_matrix], trainable=False)(_input)\n#     conv_blocks = []\n#     for sz in filter_sizes:\n#         conv = Conv1D(filters=10, kernel_size=sz, padding='same')(x)\n#         conv = MaxPooling1D(2)(conv)\n#         conv = Flatten()(conv)\n#         conv_blocks.append(conv)\n#     x = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n#     x = Dense(500)(x)\n#     x = Dropout(0.1)(x)\n#     x = Dense(50)(x)\n#     x = Dropout(0.2)(x)\n#     x = Dense(1, activation='sigmoid')(x)\n\n#     model = Model(inputs=_input, outputs=x)\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n#     model.fit(train_X[train], train_y[train], epochs=5, batch_size=512, validation_data=(train_X[test], train_y[test]))\n#     pred_vals = model.predict(train_X[test])\n#     y_te = (np.array(pred_vals) > 0.2).astype(np.int)\n#     y_te = y_te.ravel()\n#     print(\"f1-score:%f\" % f1_score(y_te, train_y[test]))\n    \n#     pred_vals = model.predict(val_X)\n# #     y_te = (np.array(pred_test) > 0.2).astype(np.int)\n#     results.append(pred_vals.ravel())\n# pred_vals = np.sum(np.array(results),axis=0)\n# y_te = ((np.array(pred_vals)/k) > 0.3).astype(np.int)\n# print(\"f1-score:%f\" % f1_score(y_te, val_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83e16b683b23fb7e5b2c94c24cfac3e71561e825"},"cell_type":"markdown","source":"### train"},{"metadata":{"trusted":true,"_uuid":"8a4ab352169a83fa45c4e24366f8ba6cc6c1926e"},"cell_type":"code","source":"#计算验证集合分数\nfrom keras import initializers, regularizers, constraints\nfrom keras import backend as K\n\nmodel = simple_CNN()\n# model = concat_CNN()\nmodel.fit(train_X, train_y, batch_size=512, epochs=5, validation_data=(val_X, val_y))\npred_vals = model.predict(val_X)\nbest_thresh = 0.5\nbest_score = 0.0\nfor thresh in tqdm(np.arange(0.1, 0.501, 0.01)):\n    thresh = np.round(thresh, 2)\n    y_te = (np.array(pred_vals) > thresh).astype(np.int)\n    y_te = y_te.ravel()\n    score = f1_score(y_te, val_y)\n    if score > best_score:\n        best_thresh = thresh\n        best_score = score\nprint(\"Val best_thresh: {:.4f}\".format(best_thresh))\nprint(\"Val F1 Score: {:.4f}\".format(best_score))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fd402d9d69890a2d1a983115e10ddd070c6c082"},"cell_type":"markdown","source":"## 提交文件"},{"metadata":{"trusted":true,"_uuid":"6d85689405e33814c4a5dd88089a167762841393"},"cell_type":"code","source":"print(\"[Step 4] Submit Results...\")\n\nall_preds = model.predict(test_X)\ny_te = (np.array(all_preds) > thresh).astype(np.int)\n\nsubmit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te.ravel()})\nsubmit_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8c02386b20e55af58d86d4df6a2429712cd19ff"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e688eb56f89c65ddb89686e99d9e9b66883b48f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}